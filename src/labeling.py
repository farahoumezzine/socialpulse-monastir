"""
===========================================================
Ce module prÃ©-labelle les posts en utilisant :
1. Sentiment des emojis (dÃ©jÃ  extrait)
2. Mots-clÃ©s positifs/nÃ©gatifs en Darija
3. RÃ¨gles linguistiques


"""

import json
import os
import re
from collections import Counter

# ============================================
# 1. DICTIONNAIRES DE MOTS-CLÃ‰S DARIJA
# ============================================

# Mots positifs en Darija tunisien
POSITIVE_WORDS = {
    # Expressions de joie/satisfaction
    'rawaa', 'hbel', 'heyel', 'momtez', 'behi', 'mezyen', 'bnin', 
    'farhan', 'farhana', 'mlih', 'temem', 'barcha behi', 'top',
    
    # QualitÃ© positive
    'jmil', 'jmila', 'hlou', 'hlowa', 'zin', 'zwina', 'raia',
    
    # SuccÃ¨s/RÃ©ussite
    'rebeh', 'najeh', 'bravo', 'mabrouk', 'tahya', 'yaaychek',
    
    # Gratitude
    'chokr', 'saha', 'merci', 'baraka', 'hamdoullah',
    
    # Ambiance positive
    'jaw', 'ambiance', 'hafla', 'fete', 'festival', 'concert',
    
    # Amour/Affection
    'hob', 'nheb', 'nhebek', 'nhebkom', 'habibi', 'habibti',
    
    # Recommandation
    'nchourek', 'nemchiwlou', 'lazem', 'worth it', 'yestehel',
    
    # Autres positifs
    'behia', 'skhoun', 'nar', 'kwi', 'fort', 'super', 'extra',
}

# Mots nÃ©gatifs en Darija tunisien
NEGATIVE_WORDS = {
    # Expressions de mÃ©contentement
    'khayeb', 'khayba', 'fdhiha', 'kharba', 'mochkla', 'machakel',
    
    # QualitÃ© nÃ©gative
    'mouch behi', 'mouch mlih', 'dhaif', 'nkes', 'wahel',
    
    # Ã‰motions nÃ©gatives
    'hzin', 'hzina', 'zaalet', 'metghachech', 'taab', 'taaba',
    'mokref', 'yekref', 'kalekni',
    
    # ProblÃ¨mes
    'panne', 'kass', 'kassat', 'taatlet', 'msakra', 'mahbous',
    'zahma', 'retard', 'takhir', 'ghyab',
    
    # Critique
    'skandal', 'aib', 'hchouma', 'karhba', 'fawdha',
    
    # Service mauvais
    'ikalek', 'mzaej', 'sot ali', 'bruit', 'sale', 'wsekh',
    
    # DÃ©ception
    'khab amli', 'makanch', 'mafamech', 'deception', 'dommage',
    
    # Autres nÃ©gatifs  
    'ghali', 'yesrek', 'voleur', 'arnaque', 'nØµab',
}

# Mots neutres/informatifs (pas de sentiment clair)
NEUTRAL_WORDS = {
    'lyoum', 'ghodwa', 'lbereh', 'tawa', 'wakteh', 'saa',
    'win', 'kifech', 'chkoun', 'chnowa', 'alech',
    'mestir', 'monastir', 'khniss', 'stade', 'corniche',
    'match', 'film', 'ardh', 'maaredh', 'nadwa',
}

# Intensificateurs (amplifient le sentiment)
INTENSIFIERS = {
    'barcha':  1.5,      # beaucoup
    'yesser': 1.5,      # trÃ¨s
    'aalekher': 1.8,    # au max
    'bel kol': 1.6,     # complÃ¨tement
    'jaw': 1.3,         # ambiance (amplifie)
    'chwaya': 0.7,      # un peu (rÃ©duit)
    'mouch barcha': 0.6,  # pas beaucoup
}

# NÃ©gateurs (inversent le sentiment)
NEGATORS = {
    'mouch', 'mech', 'ma', 'mafamech', 'makanch', 
    'jamais', 'abadan', 'la', 'non', 'bla',
}


# ============================================
# 2. FONCTION DE SCORING AUTOMATIQUE
# ============================================

def calculate_text_sentiment_score(clean_text):
    """
    Calcule un score de sentiment basÃ© sur les mots-clÃ©s. 
    
    Args:
        clean_text: Texte nettoyÃ© en Darija latin
    
    Returns:
        dict: {
            'score': float (-1 Ã  1),
            'positive_words': list,
            'negative_words': list,
            'has_negator': bool,
            'intensifier': float
        }
    """
    if not clean_text:
        return {
            'score': 0,
            'positive_words':  [],
            'negative_words':  [],
            'has_negator': False,
            'intensifier': 1.0
        }
    
    words = clean_text.lower().split()
    
    # Trouver les mots positifs et nÃ©gatifs
    found_positive = [w for w in words if w in POSITIVE_WORDS]
    found_negative = [w for w in words if w in NEGATIVE_WORDS]
    
    # VÃ©rifier les nÃ©gateurs
    has_negator = any(w in NEGATORS for w in words)
    
    # Calculer l'intensificateur moyen
    intensifier = 1.0
    for word in words:
        if word in INTENSIFIERS:
            intensifier *= INTENSIFIERS[word]
    
    # Calculer le score
    positive_count = len(found_positive)
    negative_count = len(found_negative)
    total_sentiment_words = positive_count + negative_count
    
    if total_sentiment_words == 0:
        score = 0
    else: 
        score = (positive_count - negative_count) / total_sentiment_words
    
    # Appliquer l'intensificateur
    score = score * intensifier
    
    # Inverser si nÃ©gateur prÃ©sent
    if has_negator and abs(score) > 0:
        score = -score * 0.8  # Inversion partielle
    
    # Normaliser entre -1 et 1
    score = max(-1, min(1, score))
    
    return {
        'score': round(score, 3),
        'positive_words':  found_positive,
        'negative_words': found_negative,
        'has_negator': has_negator,
        'intensifier': round(intensifier, 2)
    }


def calculate_emoji_sentiment_score(emoji_sentiment):
    """
    Extrait le score de sentiment des emojis.
    
    Args:
        emoji_sentiment:  Dict contenant les donnÃ©es emoji du preprocessing
    
    Returns:
        float: Score entre -1 et 1
    """
    if not emoji_sentiment or emoji_sentiment. get('emoji_count', 0) == 0:
        return 0
    
    return emoji_sentiment. get('avg_score', 0)


def combine_sentiment_scores(text_score, emoji_score, text_weight=0.6, emoji_weight=0.4):
    """
    Combine les scores de sentiment du texte et des emojis.
    
    Args:
        text_score: Score basÃ© sur les mots-clÃ©s
        emoji_score: Score basÃ© sur les emojis
        text_weight: Poids du score texte (dÃ©faut: 0.6)
        emoji_weight: Poids du score emoji (dÃ©faut: 0.4)
    
    Returns:
        float: Score combinÃ© entre -1 et 1
    """
    # Si pas d'emojis, utiliser seulement le texte
    if emoji_score == 0:
        return text_score
    
    # Si pas de mots-clÃ©s sentiment, utiliser seulement les emojis
    if text_score == 0:
        return emoji_score
    
    # Combinaison pondÃ©rÃ©e
    combined = (text_score * text_weight) + (emoji_score * emoji_weight)
    
    return round(combined, 3)


def score_to_label(score, thresholds=None):
    """
    Convertit un score numÃ©rique en label catÃ©goriel.
    
    Args:
        score: Score entre -1 et 1
        thresholds: Dict avec seuils personnalisÃ©s
    
    Returns:
        str: 'positive', 'negative', ou 'neutral'
    """
    if thresholds is None: 
        thresholds = {
            'positive': 0.2,   # score >= 0.2 â†’ positif
            'negative': -0.2,  # score <= -0.2 â†’ nÃ©gatif
        }
    
    if score >= thresholds['positive']:
        return 'positive'
    elif score <= thresholds['negative']: 
        return 'negative'
    else:
        return 'neutral'


def calculate_confidence(text_analysis, emoji_sentiment, final_score):
    """
    Calcule un niveau de confiance pour le label attribuÃ©.
    
    Returns:
        float: Confiance entre 0 et 1
    """
    confidence = 0.5  # Base
    
    # Plus de mots-clÃ©s trouvÃ©s = plus de confiance
    sentiment_words = len(text_analysis['positive_words']) + len(text_analysis['negative_words'])
    if sentiment_words >= 3:
        confidence += 0.2
    elif sentiment_words >= 1:
        confidence += 0.1
    
    # Emojis prÃ©sents = plus de confiance
    emoji_count = emoji_sentiment.get('emoji_count', 0) if emoji_sentiment else 0
    if emoji_count >= 2:
        confidence += 0.15
    elif emoji_count >= 1:
        confidence += 0.1
    
    # Score fort = plus de confiance
    if abs(final_score) >= 0.5:
        confidence += 0.15
    elif abs(final_score) >= 0.3:
        confidence += 0.1
    
    # Accord texte/emoji = plus de confiance
    text_score = text_analysis['score']
    emoji_score = calculate_emoji_sentiment_score(emoji_sentiment)
    if text_score != 0 and emoji_score != 0:
        if (text_score > 0 and emoji_score > 0) or (text_score < 0 and emoji_score < 0):
            confidence += 0.1  # Accord
        else:
            confidence -= 0.1  # DÃ©saccord
    
    return round(min(1.0, max(0.0, confidence)), 2)


# ============================================
# 3. FONCTION PRINCIPALE DE LABELING
# ============================================

def label_post(post):
    """
    Labelle automatiquement un post avec son sentiment. 
    
    Args:
        post: Dict contenant 'clean_text' et 'emoji_sentiment'
    
    Returns:
        dict: Post enrichi avec le label de sentiment
    """
    clean_text = post. get('clean_text', '')
    emoji_sentiment = post.get('emoji_sentiment', {})
    
    # 1. Analyser le texte
    text_analysis = calculate_text_sentiment_score(clean_text)
    
    # 2. Obtenir le score emoji
    emoji_score = calculate_emoji_sentiment_score(emoji_sentiment)
    
    # 3. Combiner les scores
    final_score = combine_sentiment_scores(text_analysis['score'], emoji_score)
    
    # 4. Convertir en label
    label = score_to_label(final_score)
    
    # 5. Calculer la confiance
    confidence = calculate_confidence(text_analysis, emoji_sentiment, final_score)
    
    # 6. Enrichir le post
    labeled_post = post.copy()
    labeled_post['sentiment_analysis'] = {
        'label': label,
        'score': final_score,
        'confidence': confidence,
        'text_analysis': {
            'score': text_analysis['score'],
            'positive_words': text_analysis['positive_words'],
            'negative_words': text_analysis['negative_words'],
            'has_negator': text_analysis['has_negator'],
            'intensifier':  text_analysis['intensifier'],
        },
        'emoji_score': emoji_score,
        'needs_review': confidence < 0.6,  # Flag pour rÃ©vision manuelle
    }
    
    return labeled_post


# ============================================
# 4. TRAITEMENT DU DATASET COMPLET
# ============================================

def label_dataset(input_path, output_path):
    """
    Labelle tout le dataset et sauvegarde les rÃ©sultats.
    
    Args:
        input_path:  Chemin vers le fichier JSON preprocessÃ©
        output_path:  Chemin vers le fichier de sortie labellisÃ©
    """
    print("=" * 70)
    print("ğŸ·ï¸  SOCIALPULSE MONASTIR - Labeling Semi-Automatique")
    print("=" * 70)
    
    # Charger les donnÃ©es
    print(f"\nğŸ“‚ Chargement des donnÃ©es depuis:  {input_path}")
    if not os.path.exists(input_path):
        print(f"âŒ Erreur: Fichier non trouvÃ©:  {input_path}")
        return None
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… {len(data)} posts chargÃ©s")
    
    # Labeller chaque post
    print(f"\nğŸ”„ Labeling en cours...")
    labeled_data = []
    
    stats = {
        'total':  len(data),
        'positive': 0,
        'negative': 0,
        'neutral': 0,
        'high_confidence': 0,
        'needs_review': 0,
    }
    
    for i, post in enumerate(data):
        labeled_post = label_post(post)
        labeled_data.append(labeled_post)
        
        # Mise Ã  jour des stats
        label = labeled_post['sentiment_analysis']['label']
        stats[label] += 1
        
        confidence = labeled_post['sentiment_analysis']['confidence']
        if confidence >= 0.7:
            stats['high_confidence'] += 1
        if labeled_post['sentiment_analysis']['needs_review']: 
            stats['needs_review'] += 1
        
        # Progression
        if (i + 1) % 20 == 0:
            print(f"   TraitÃ©:  {i + 1}/{len(data)} posts")
    
    # Sauvegarder
    print(f"\nğŸ’¾ Sauvegarde vers: {output_path}")
    
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path. exists(output_dir):
        os.makedirs(output_dir)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(labeled_data, f, ensure_ascii=False, indent=2)
    
    # Afficher les statistiques
    print("\n" + "=" * 70)
    print("ğŸ“Š STATISTIQUES DE LABELING")
    print("=" * 70)
    
    print(f"\nğŸ“Œ Total posts:  {stats['total']}")
    
    print(f"\nğŸ“Š RÃ©partition des sentiments:")
    for sentiment in ['positive', 'negative', 'neutral']:
        count = stats[sentiment]
        pct = (count / stats['total']) * 100
        bar = "â–ˆ" * int(pct / 2)
        emoji = {'positive': 'âœ…', 'negative': 'âŒ', 'neutral': 'âšª'}[sentiment]
        print(f"   {emoji} {sentiment. capitalize():10}:  {count:4} ({pct: 5.1f}%) {bar}")
    
    print(f"\nğŸ¯ Confiance:")
    print(f"   â€¢ Haute confiance (â‰¥0.7): {stats['high_confidence']} ({stats['high_confidence']/stats['total']*100:.1f}%)")
    print(f"   â€¢ Ã€ rÃ©viser (<0.6):       {stats['needs_review']} ({stats['needs_review']/stats['total']*100:.1f}%)")
    
    # Exemples
    print("\n" + "=" * 70)
    print("ğŸ“ EXEMPLES DE LABELING")
    print("=" * 70)
    
    # Un exemple de chaque catÃ©gorie
    examples = {'positive': None, 'negative': None, 'neutral': None}
    for post in labeled_data:
        label = post['sentiment_analysis']['label']
        if examples[label] is None: 
            examples[label] = post
        if all(v is not None for v in examples. values()):
            break
    
    for sentiment, post in examples.items():
        if post: 
            print(f"\n{'â”€' * 70}")
            emoji = {'positive': 'âœ…', 'negative': 'âŒ', 'neutral': 'âšª'}[sentiment]
            print(f"{emoji} {sentiment.upper()}")
            print(f"{'â”€' * 70}")
            print(f"ğŸ“ Texte: {post. get('clean_text', '')[:80]}...")
            sa = post['sentiment_analysis']
            print(f"ğŸ“Š Score: {sa['score']} | Confiance: {sa['confidence']}")
            if sa['text_analysis']['positive_words']:
                print(f"   âœ… Mots positifs: {sa['text_analysis']['positive_words']}")
            if sa['text_analysis']['negative_words']:
                print(f"   âŒ Mots nÃ©gatifs: {sa['text_analysis']['negative_words']}")
    
    print("\n" + "=" * 70)
    print("âœ… LABELING TERMINÃ‰!")
    print("=" * 70)
    
    return labeled_data, stats


# ============================================
# 5. OUTILS DE RÃ‰VISION MANUELLE
# ============================================

def get_posts_for_review(labeled_data, max_posts=50):
    """
    RÃ©cupÃ¨re les posts qui nÃ©cessitent une rÃ©vision manuelle.
    
    Args:
        labeled_data: Liste des posts labellisÃ©s
        max_posts: Nombre maximum de posts Ã  rÃ©viser
    
    Returns: 
        list: Posts Ã  rÃ©viser, triÃ©s par confiance croissante
    """
    needs_review = [
        post for post in labeled_data 
        if post['sentiment_analysis']['needs_review']
    ]
    
    # Trier par confiance croissante (les moins sÃ»rs en premier)
    needs_review.sort(key=lambda x: x['sentiment_analysis']['confidence'])
    
    return needs_review[:max_posts]


def export_for_manual_review(labeled_data, output_path, max_posts=100):
    """
    Exporte les posts Ã  rÃ©viser dans un format simple pour annotation manuelle.
    
    Args:
        labeled_data: Liste des posts labellisÃ©s
        output_path: Chemin du fichier de sortie
        max_posts: Nombre maximum de posts Ã  exporter
    """
    posts_to_review = get_posts_for_review(labeled_data, max_posts)
    
    print(f"\nğŸ“ Export de {len(posts_to_review)} posts pour rÃ©vision manuelle...")
    
    # Format simplifiÃ© pour rÃ©vision
    review_data = []
    for i, post in enumerate(posts_to_review):
        review_item = {
            'id': post. get('id', i),
            'original_text': post.get('original_text', post.get('text', '')),
            'clean_text': post.get('clean_text', ''),
            'auto_label': post['sentiment_analysis']['label'],
            'auto_score': post['sentiment_analysis']['score'],
            'confidence':  post['sentiment_analysis']['confidence'],
            'positive_words': post['sentiment_analysis']['text_analysis']['positive_words'],
            'negative_words':  post['sentiment_analysis']['text_analysis']['negative_words'],
            # Champ Ã  remplir manuellement
            'manual_label': '',  # positive, negative, neutral
            'reviewer_notes': '',
        }
        review_data.append(review_item)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(review_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Fichier de rÃ©vision exportÃ©: {output_path}")
    print(f"   â†’ Ouvrez ce fichier et remplissez 'manual_label' pour chaque post")
    
    return review_data


def merge_manual_labels(labeled_data, review_path):
    """
    Fusionne les labels manuels avec le dataset labellisÃ©.
    
    Args:
        labeled_data:  Liste des posts labellisÃ©s automatiquement
        review_path:  Chemin vers le fichier avec les labels manuels
    
    Returns:
        list: Dataset avec labels corrigÃ©s
    """
    print(f"\nğŸ”„ Fusion des labels manuels...")
    
    with open(review_path, 'r', encoding='utf-8') as f:
        reviewed = json.load(f)
    
    # CrÃ©er un mapping id -> manual_label
    manual_labels = {
        item['id']: item['manual_label']
        for item in reviewed
        if item. get('manual_label')  # Seulement si rempli
    }
    
    # Appliquer les corrections
    corrections = 0
    for post in labeled_data:
        post_id = post. get('id')
        if post_id in manual_labels:
            old_label = post['sentiment_analysis']['label']
            new_label = manual_labels[post_id]
            if old_label != new_label: 
                post['sentiment_analysis']['label'] = new_label
                post['sentiment_analysis']['manually_corrected'] = True
                post['sentiment_analysis']['original_auto_label'] = old_label
                corrections += 1
    
    print(f"âœ… {corrections} labels corrigÃ©s manuellement")
    
    return labeled_data


# ============================================
# 6. GÃ‰NÃ‰RATION DU DATASET FINAL
# ============================================

def generate_training_dataset(labeled_data, output_path, min_confidence=0.5):
    """
    GÃ©nÃ¨re le dataset final pour l'entraÃ®nement du modÃ¨le.
    
    Args:
        labeled_data:  Liste des posts labellisÃ©s
        output_path: Chemin de sortie
        min_confidence:  Confiance minimale pour inclure un post
    
    Returns:
        dict:  Statistiques du dataset gÃ©nÃ©rÃ©
    """
    print("\n" + "=" * 70)
    print("ğŸ“¦ GÃ‰NÃ‰RATION DU DATASET D'ENTRAÃNEMENT")
    print("=" * 70)
    
    # Filtrer par confiance
    training_data = []
    for post in labeled_data:
        confidence = post['sentiment_analysis']['confidence']
        manually_corrected = post['sentiment_analysis']. get('manually_corrected', False)
        
        # Inclure si confiance suffisante OU corrigÃ© manuellement
        if confidence >= min_confidence or manually_corrected:
            training_item = {
                'id': post.get('id'),
                'text': post.get('clean_text', ''),
                'label': post['sentiment_analysis']['label'],
                'confidence': confidence,
                'source': 'manual' if manually_corrected else 'auto',
            }
            training_data.append(training_item)
    
    # Statistiques
    stats = {
        'total': len(training_data),
        'positive': sum(1 for x in training_data if x['label'] == 'positive'),
        'negative': sum(1 for x in training_data if x['label'] == 'negative'),
        'neutral': sum(1 for x in training_data if x['label'] == 'neutral'),
        'from_manual': sum(1 for x in training_data if x['source'] == 'manual'),
        'from_auto': sum(1 for x in training_data if x['source'] == 'auto'),
    }
    
    # Sauvegarder
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Dataset d'entraÃ®nement gÃ©nÃ©rÃ©: {output_path}")
    print(f"\nğŸ“Š Statistiques:")
    print(f"   â€¢ Total: {stats['total']} posts")
    print(f"   â€¢ Positif: {stats['positive']} ({stats['positive']/stats['total']*100:.1f}%)")
    print(f"   â€¢ NÃ©gatif: {stats['negative']} ({stats['negative']/stats['total']*100:.1f}%)")
    print(f"   â€¢ Neutre:  {stats['neutral']} ({stats['neutral']/stats['total']*100:.1f}%)")
    print(f"   â€¢ Labels auto:  {stats['from_auto']}")
    print(f"   â€¢ Labels manuels: {stats['from_manual']}")
    
    return training_data, stats




if __name__ == "__main__":
    import sys
    
    # Chemins
    script_dir = os. path.dirname(os.path. abspath(__file__))
    project_root = os.path. dirname(script_dir)
    
    # Fichiers
    input_file = os.path. join(project_root, 'data', 'processed', 'result_after_validation.json')
    labeled_file = os. path.join(project_root, 'data', 'processed', 'labeled_data.json')
    review_file = os. path.join(project_root, 'data', 'processed', 'posts_to_review.json')
    training_file = os.path.join(project_root, 'data', 'processed', 'training_dataset.json')
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MENU INTERACTIF
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("=" * 70)
    print("ğŸ·ï¸  SOCIALPULSE MONASTIR - SystÃ¨me de Labeling")
    print("=" * 70)
    print("""
Choisissez une option: 

  1ï¸âƒ£  Labeling automatique (premiÃ¨re fois)
      â†’ GÃ©nÃ¨re les labels automatiques et le fichier de rÃ©vision
      
  2ï¸âƒ£  Fusionner les labels manuels (aprÃ¨s rÃ©vision)
      â†’ Fusionne vos corrections avec le dataset
      
  3ï¸âƒ£  GÃ©nÃ©rer le dataset d'entraÃ®nement
      â†’ CrÃ©e le fichier final pour l'entraÃ®nement
    """)
    
    choice = input("Votre choix (1, 2, ou 3): ").strip()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OPTION 1: Labeling automatique (premiÃ¨re fois)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if choice == "1":
        print("\nâš ï¸  ATTENTION:  Ceci va Ã©craser les fichiers existants!")
        confirm = input("Continuer? (oui/non): ").strip().lower()
        
        if confirm in ['oui', 'o', 'yes', 'y']:
            labeled_data, stats = label_dataset(input_file, labeled_file)
            if labeled_data: 
                export_for_manual_review(labeled_data, review_file, max_posts=50)
                print("\nâœ… Labeling terminÃ©!")
                print(f"ğŸ“ Maintenant, ouvrez et remplissez:  {review_file}")
                print("   Puis relancez avec l'option 2")
        else:
            print("âŒ AnnulÃ©.")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OPTION 2: Fusionner les labels manuels
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    elif choice == "2":
        print("\n" + "=" * 70)
        print("ğŸ”„ FUSION DES LABELS MANUELS")
        print("=" * 70)
        
        # VÃ©rifier que les fichiers existent
        if not os.path.exists(labeled_file):
            print(f"âŒ Erreur: Fichier non trouvÃ©: {labeled_file}")
            print("   â†’ ExÃ©cutez d'abord l'option 1")
            sys.exit(1)
        
        if not os.path.exists(review_file):
            print(f"âŒ Erreur: Fichier non trouvÃ©: {review_file}")
            print("   â†’ ExÃ©cutez d'abord l'option 1")
            sys.exit(1)
        
        # Charger les donnÃ©es labellisÃ©es automatiquement
        print(f"\nğŸ“‚ Chargement du dataset labellisÃ©: {labeled_file}")
        with open(labeled_file, 'r', encoding='utf-8') as f:
            labeled_data = json.load(f)
        print(f"âœ… {len(labeled_data)} posts chargÃ©s")
        
        # Afficher un aperÃ§u du fichier de rÃ©vision
        print(f"\nğŸ“‚ VÃ©rification du fichier de rÃ©vision: {review_file}")
        with open(review_file, 'r', encoding='utf-8') as f:
            review_data = json.load(f)
        
        # Compter les labels manuels remplis
        filled_labels = [r for r in review_data if r. get('manual_label')]
        print(f"âœ… {len(review_data)} posts Ã  rÃ©viser")
        print(f"âœï¸  {len(filled_labels)} labels manuels remplis")
        
        if len(filled_labels) == 0:
            print("\nâš ï¸  ATTENTION: Aucun label manuel trouvÃ©!")
            print(f"   â†’ Ouvrez le fichier: {review_file}")
            print("   â†’ Remplissez le champ 'manual_label' pour chaque post")
            print("   â†’ Valeurs possibles: 'positive', 'negative', 'neutral'")
            print("\nğŸ“ Exemple de ce qu'il faut faire:")
            print('''
    {
        "id": 5,
        "auto_label": "neutral",
        "manual_label": "",          â† CHANGEZ EN:  "positive" ou "negative" ou "neutral"
        ... 
    }
            ''')
            sys.exit(1)
        
        # Afficher les corrections Ã  appliquer
        print("\nğŸ“‹ Corrections Ã  appliquer:")
        for item in filled_labels[: 5]:  # Afficher les 5 premiers
            print(f"   ID {item['id']}: {item['auto_label']} â†’ {item['manual_label']}")
        if len(filled_labels) > 5:
            print(f"   ... et {len(filled_labels) - 5} autres")
        
        # Fusionner avec les labels manuels
        labeled_data = merge_manual_labels(labeled_data, review_file)
        
        # Sauvegarder le dataset corrigÃ©
        print(f"\nğŸ’¾ Sauvegarde du dataset corrigÃ©: {labeled_file}")
        with open(labeled_file, 'w', encoding='utf-8') as f:
            json.dump(labeled_data, f, ensure_ascii=False, indent=2)
        
        # Statistiques
        print("\nâœ… Fusion terminÃ©e!")
        print("   â†’ Relancez avec l'option 3 pour gÃ©nÃ©rer le dataset d'entraÃ®nement")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OPTION 3: GÃ©nÃ©rer le dataset d'entraÃ®nement
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    elif choice == "3":
        print("\n" + "=" * 70)
        print("ğŸ“¦ GÃ‰NÃ‰RATION DU DATASET D'ENTRAÃNEMENT")
        print("=" * 70)
        
        # VÃ©rifier que le fichier existe
        if not os.path.exists(labeled_file):
            print(f"âŒ Erreur:  Fichier non trouvÃ©:  {labeled_file}")
            print("   â†’ ExÃ©cutez d'abord l'option 1")
            sys.exit(1)
        
        # Charger les donnÃ©es
        print(f"\nğŸ“‚ Chargement du dataset labellisÃ©: {labeled_file}")
        with open(labeled_file, 'r', encoding='utf-8') as f:
            labeled_data = json.load(f)
        print(f"âœ… {len(labeled_data)} posts chargÃ©s")
        
        # GÃ©nÃ©rer le dataset d'entraÃ®nement
        training_data, stats = generate_training_dataset(labeled_data, training_file, min_confidence=0.5)
        
        # Statistiques finales
        print("\n" + "=" * 70)
        print("ğŸ“Š STATISTIQUES FINALES")
        print("=" * 70)
        
        manual_corrections = sum(
            1 for post in labeled_data 
            if post['sentiment_analysis']. get('manually_corrected', False)
        )
        
        print(f"\nâœï¸  Corrections manuelles:  {manual_corrections}")
        
        final_stats = {'positive': 0, 'negative':  0, 'neutral': 0}
        for post in labeled_data:
            label = post['sentiment_analysis']['label']
            final_stats[label] += 1
        
        print(f"\nğŸ“Š RÃ©partition finale:")
        total = len(labeled_data)
        for sentiment in ['positive', 'negative', 'neutral']:
            count = final_stats[sentiment]
            pct = (count / total) * 100
            bar = "â–ˆ" * int(pct / 2)
            emoji = {'positive': 'âœ…', 'negative': 'âŒ', 'neutral': 'âšª'}[sentiment]
            print(f"   {emoji} {sentiment.capitalize():10}:  {count: 4} ({pct: 5.1f}%) {bar}")
        
        print("\n" + "=" * 70)
        print("âœ… DATASET PRÃŠT POUR L'ENTRAÃNEMENT!")
        print("=" * 70)
        print(f"\nğŸ“ Fichier gÃ©nÃ©rÃ©: {training_file}")
        print("\nğŸš€ Prochaine Ã©tape: python src/train.py")
    
    else:
        print("âŒ Option invalide.  Choisissez 1, 2, ou 3.")